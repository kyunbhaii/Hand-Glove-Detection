# üß§ Glove vs Bare Hand Detection

This project detects **gloved** and **bare hands** in images using a custom-trained YOLOv8 model.

---

## üìÅ Folder Structure

```
‚îú‚îÄ‚îÄ Glove_Detection.ipynb
‚îú‚îÄ‚îÄ CLI_inference.py
‚îÇ
‚îú‚îÄ‚îÄ runs/
‚îÇ   ‚îî‚îÄ‚îÄ glove_vs_bare_yolov8n/
‚îÇ       ‚îî‚îÄ‚îÄ weights/
‚îÇ           ‚îú‚îÄ‚îÄ best.pt
‚îÇ           ‚îî‚îÄ‚îÄ last.pt
‚îÇ
‚îú‚îÄ‚îÄ output/
‚îÇ
‚îú‚îÄ‚îÄ logs/
‚îÇ
‚îî‚îÄ‚îÄ README.md
```

---

## üöÄ How to Run Inference via CLI

### 1Ô∏è‚É£ Requirements

Install dependencies (Python ‚â• 3.11 recommended):
```bash
pip install ultralytics opencv-python-headless torch torchvision torchaudio
```

---

### 2Ô∏è‚É£ Command Example

Run inference directly from the terminal:
```bash
python CLI_inference.py \
  -i "path/to/input_images" \
  -o "path/to/output_folder" \
  -l "path/to/logs_folder" \
  -c 0.25
```

**Arguments**
| Flag | Description |
|------|--------------|
| `-i` or `--input` | Path to folder containing input images |
| `-o` or `--output` | Folder to save annotated output images |
| `-l` or `--logs` | Folder to save JSON detection logs |
| `-c` or `--confidence` | Confidence threshold (default: 0.25) |

---

## üß† About the Model

- **Model:** YOLOv8n (fine-tuned on glove vs bare hand dataset)  
- **Classes:** `glove_hand`, `bare_hand`  
- **Framework:** Ultralytics YOLOv8  
- **Dataset:** Exported from Roboflow, annotated for binary classification of hand states

---

## üìä Dataset Details

- **Dataset name:** Glove-Hand-and-Bare-Hand  
- **Source:** [Roboflow Dataset ‚Üó](https://app.roboflow.com/glove-detection-3vldq/glove-hand-and-bare-hand-zwvif/1)  
- **Classes:** `glove_hand`, `bare_hand` 
- **Split:**  
  - Train ‚Üí 82%  
  - Validation ‚Üí 16%  
  - Test ‚Üí 2%  
- **Format:** YOLOv8 (images + labels + `data.yaml`)

---

## ‚öôÔ∏è Advanced Features

### üß© Image Augmentation  
To make the model more **robust** and capable of handling diverse lighting conditions and hand poses, multiple image augmentations were applied during YOLOv8 training.

**Augmentations used:**
- Random horizontal flip (`fliplr=0.5`)  
- Hue-Saturation-Value adjustment (`hsv_h=0.015, hsv_s=0.7, hsv_v=0.4`)  
- Random scaling and translation (`scale=0.5`, `translate=0.1`)  
- Random erasing and mosaic augmentations  

> These augmentations improve generalization, ensuring the model performs well on unseen real-world data.

### ‚ö° Batch Inference & Multiprocessing  
The **CLI script (`CLI_inference.py`)** supports **batch inference**, allowing efficient detection on multiple images simultaneously.

When a folder is passed as input, YOLOv8 automatically loads and processes all images in batches:
```bash
python CLI_inference.py -i "path/to/test_images/" -o "path/to/output/"

---

## üí° What Worked
- Rebalanced dataset and removed unrelated ‚Äúonly glove‚Äù class ‚Üí improved mAP & recall
- Training directly from Google Drive ensured model weights were not lost between Colab sessions
- Label mapping in inference standardized JSON output as gloved_hand / bare_hand
- Smaller YOLOv8n model gave faster inference with decent accuracy

## ‚ö†Ô∏è What Didn‚Äôt Work (and Fixes)

| Issue | Cause | Fix |
|-------|--------|-----|
| Dataset not loading | Folder paths were wrong after download | Fixed file paths in `data.yaml` - Adjusted to relative paths (`../train/images`, etc.) |
| Only glove images in validation | Random split created imbalance | Rebalanced dataset in Roboflow |
| Inference was slow on CPU | YOLOv8 defaulted to CPU mode | Switched to GPU runtime for faster processing |

---

## üß© Output Format

Each inference produces:
1. Annotated image (saved to `output/`)
2. Corresponding JSON file (saved to `logs/`) with the structure:
```json
{
  "filename": "image1.jpg",
  "detections": [
    {"label": "gloved_hand", "confidence": 0.92, "bbox": [x1, y1, x2, y2]},
    {"label": "bare_hand", "confidence": 0.85, "bbox": [x1, y1, x2, y2]}
  ]
}
```

---

## üì¨ Notes
- Trained weights (`best.pt`) are located in `runs/glove_vs_bare_yolov8n/weights/`.  
- Works seamlessly with YOLOv8 for both local and Colab environments.  
- You can retrain or fine-tune using `Glove_Detection.ipynb`.
