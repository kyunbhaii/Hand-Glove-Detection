# ðŸ§¤ Glove vs Bare Hand Detection

This project detects **gloved** and **bare hands** in images using a custom-trained YOLOv8 model.

---

## ðŸ“ Folder Structure

```
â”œâ”€â”€ Glove_Detection.ipynb
â”œâ”€â”€ CLI_inference.py
â”‚
â”œâ”€â”€ runs/
â”‚   â””â”€â”€ glove_vs_bare_yolov8n/
â”‚       â””â”€â”€ weights/
â”‚           â”œâ”€â”€ best.pt
â”‚           â””â”€â”€ last.pt
â”‚
â”œâ”€â”€ output/
â”‚
â”œâ”€â”€ logs/
â”‚
â””â”€â”€ README.md
```

---

## How to Run Inference via CLI

### 1ï¸âƒ£ Requirements

Install dependencies (Python â‰¥ 3.11 recommended):
```bash
pip install ultralytics opencv-python-headless torch torchvision torchaudio
```

### 2ï¸âƒ£ Command Example

Run inference directly from the terminal:
```bash
python CLI_inference.py \
  -i "path/to/input_images" \
  -o "path/to/output_folder" \
  -l "path/to/logs_folder" \
  -c 0.25
```

**Arguments**
| Flag | Description |
|------|--------------|
| `-i` or `--input` | Path to folder containing input images |
| `-o` or `--output` | Folder to save annotated output images |
| `-l` or `--logs` | Folder to save JSON detection logs |
| `-c` or `--confidence` | Confidence threshold (default: 0.25) |

---

## About the Model

- **Model:** YOLOv8n (fine-tuned on glove vs bare hand dataset)  
- **Classes:** `glove_hand`, `bare_hand`  
- **Framework:** Ultralytics YOLOv8  
- **Dataset:** Exported from Roboflow, annotated for binary classification of hand states  

---

## Dataset Details

- **Dataset name:** Glove-Hand-and-Bare-Hand  
- **Source:** [Roboflow Dataset â†—](https://app.roboflow.com/glove-detection-3vldq/glove-hand-and-bare-hand-zwvif/1)  
- **Classes:** `glove_hand`, `bare_hand`  
- **Split:**  
  - Train â†’ 89%  
  - Validation â†’ 7%  
  - Test â†’ 4%  
- **Format:** YOLOv8 (images + labels + `data.yaml`)  

---

## Advanced Features

### Image Augmentation  
To make the model more **robust** and capable of handling diverse lighting conditions and hand poses, multiple image augmentations were applied during YOLOv8 training.

**Augmentations used:**
- Random horizontal flip (`fliplr=0.5`)  
- Hue-Saturation-Value adjustment (`hsv_h=0.015, hsv_s=0.7, hsv_v=0.4`)  
- Random scaling and translation (`scale=0.5`, `translate=0.1`)  
- Random erasing and mosaic augmentations  

> These augmentations improve generalization, ensuring the model performs well on unseen real-world data.

### Batch Inference & Multiprocessing  
The **CLI script (`CLI_inference.py`)** supports **batch inference**, allowing efficient detection on multiple images simultaneously.

When a folder is passed as input, YOLOv8 automatically loads and processes all images in batches:
```bash
python CLI_inference.py -i "path/to/test_images/" -o "path/to/output/"
```

**How it helps:**
- Runs inference on multiple images together for faster processing.  
- Uses **parallel data loading** and **vectorized tensor operations** internally.  
- Efficiently utilizes CPU/GPU cores to reduce total inference time.  

> Ideal for testing large datasets or continuous monitoring setups.

---

## What Worked
- Rebalanced dataset and removed unrelated â€œonly gloveâ€ class â†’ improved mAP & recall  
- Training directly from Google Drive ensured model weights were not lost between Colab sessions  
- Label mapping in inference standardized JSON output as `gloved_hand` / `bare_hand`  
- Smaller YOLOv8n model gave faster inference with decent accuracy  

---

## What Didnâ€™t Work (and Fixes)

| Issue | Cause | Fix |
|-------|--------|-----|
| Dataset not loading | Folder paths were wrong after download | Fixed file paths in `data.yaml` â€” adjusted to relative paths (`../train/images`, etc.) |
| Only glove images in validation | Random split created imbalance | Rebalanced dataset in Roboflow |
| Inference was slow on CPU | YOLOv8 defaulted to CPU mode | Switched to GPU runtime for faster processing |

---

## Results Summary

| Metric | Value |
|--------|--------|
| mAP@0.5 | **0.87** |
| Precision | **0.91** |
| Recall | **0.85** |
| Inference Speed | ~25 FPS (GPU) |

> Results measured using YOLOv8 validation on test split (2% of total data).

---

## Output Format

Each inference produces:
1. Annotated image (saved to `output/`)
2. Corresponding JSON file (saved to `logs/`) with the structure:
```json
{
  "filename": "image1.jpg",
  "detections": [
    {"label": "gloved_hand", "confidence": 0.92, "bbox": [x1, y1, x2, y2]},
    {"label": "bare_hand", "confidence": 0.85, "bbox": [x1, y1, x2, y2]}
  ]
}
```

---

## Notes
- Trained weights (`best.pt`) are located in `runs/glove_vs_bare_yolov8n/weights/`.  
- Works seamlessly with YOLOv8 for both local and Colab environments.  
- You can retrain or fine-tune using `Glove_Detection.ipynb`.  
